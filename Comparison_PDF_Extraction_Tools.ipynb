{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing All Three PDF Extraction Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to compare all three of the pdf extraction tools on the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = '/PDF_comparison/3666_Module_9_Building_Chatbots.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "# rename any files that have spaces or special characters in the name, because pdfminer can't handle it\n",
    "import re\n",
    "import os\n",
    "\n",
    "src = './PDF_comparison'\n",
    "dst = './PDF_comparison'\n",
    "# recursively walk directory structure looking for pdf files \n",
    "badchars = r\"[\\(\\)<>?!\\'\\\",\\s]+\"\n",
    "for root, dirs, files in os.walk(src):\n",
    "    for file in files:\n",
    "        path_to_pdf = os.path.join(root, file)\n",
    "        [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "        if ext == '.pdf':\n",
    "            # when a pdf file is found, check the filename for special characters\n",
    "            [fpath, fname] = os.path.split(stem)\n",
    "            if re.search(badchars, fname):\n",
    "                # if special characters found, build a new filename\n",
    "                print(\"Found \" + file)\n",
    "                dstname=re.sub(r\"[\\s]+\", \"_\", fname) \n",
    "                dstname=re.sub(badchars,\"\", dstname)\n",
    "                dstpath = os.path.join(fpath, dstname + ext) \n",
    "                print(\"Renaming to \" + dstpath)\n",
    "                # rename original pdf file to new filename in original directory\n",
    "                os.rename(path_to_pdf, dstpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./PDF_comparison/3666_Module_9_Building_Chatbots.pdf\n",
      "Writing contents to ./PDF_comparison/3666_Module_9_Building_Chatbots.txt\n"
     ]
    }
   ],
   "source": [
    "# use pdfminer in command line mode on each pdf file in the directory structure\n",
    "# this code is adapted from nadya-p/pdf_to_text.py\n",
    "import pdfminer\n",
    "import os\n",
    "\n",
    "# recursively walk directory structure looking for pdf files \n",
    "for root, dirs, files in os.walk(src):\n",
    "    for file in files:\n",
    "        path_to_pdf = os.path.join(root, file)\n",
    "        [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "        if ext == '.pdf':\n",
    "            # when a pdf file is found, construct the output path name\n",
    "            print(\"Processing \" + path_to_pdf)\n",
    "            [_, fname] = os.path.split(stem)\n",
    "            path_to_txt = os.path.join(dst, fname) + '.txt'\n",
    "            print(\"Writing contents to \" + path_to_txt)\n",
    "            # use pdfminer in command line mode to convert pdf file to text file\n",
    "            !pdf2txt.py -o {path_to_txt} {path_to_pdf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import PyPDF2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is to extract the text using pypdf2\n",
    "class PyPDF2Extract(object):\n",
    "    # initialize the class\n",
    "    def __init__(self, target_directory_name):\n",
    "        self.target = str(target_directory_name)\n",
    "        \n",
    "        \n",
    "    # define a function to extract a pdf \n",
    "    def pdfExtract(self, file):\n",
    "        # open the pdf file\n",
    "        pdf = open(file, 'rb')\n",
    "        # convert the pdf to a PdfFileReader object\n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf)\n",
    "        # check if the pdf file is encrypted\n",
    "        if read_pdf.isEncrypted == True:\n",
    "            print(file + ' file is encrypted')\n",
    "        else:\n",
    "            print(file)\n",
    "            # get the page content\n",
    "            page_content = []\n",
    "            # get the number of pages in the document\n",
    "            number_of_pages = read_pdf.getNumPages()\n",
    "            # iterate over each page to extract the text\n",
    "            for i in range(number_of_pages):\n",
    "                page = read_pdf.getPage(i)\n",
    "                # some of the files throws a TypeError\n",
    "                # others may throw a KeyError if there is a blank page\n",
    "                # this has not been addressed here\n",
    "                try:\n",
    "                    content = page.extractText()\n",
    "                    content = content.replace(\"\\n\",\" \")\n",
    "                    page_content.append(content)\n",
    "                except TypeError:\n",
    "                    pass\n",
    "                # set condition for writing the text file\n",
    "                if (i+ 1) == number_of_pages:\n",
    "                    # write the text file\n",
    "                    with open(str(os.getcwd()) + '/' + self.target + '/' + file[:-4] + '.txt', 'w') as f:\n",
    "                        f.write(str(page_content) + \"\\n\")\n",
    "                        print(file + ' success')\n",
    "    \n",
    "    def transform(self):\n",
    "        # resolve files in directory using glob\n",
    "        files = list(glob.glob(\"*.pdf\"))\n",
    "        # iterate over files to run pdfExtract function\n",
    "        for i in files:\n",
    "            #check if the target directory exists, if it doesn't create the target\n",
    "            if not os.path.exists(str(os.getcwd()) + '/' + self.target):\n",
    "                os.makedirs(str(os.getcwd()) + '/' + self.target)\n",
    "            self.pdfExtract(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(str(os.getcwd()) + '/PDF_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization the PyPDF2Extract class, specifying the target directory name\n",
    "pypdf2_extractor = PyPDF2Extract(target_directory_name = 'PDF_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3666_Module_9_Building_Chatbots.pdf\n",
      "3666_Module_9_Building_Chatbots.pdf success\n"
     ]
    }
   ],
   "source": [
    "#perform the transformation\n",
    "pypdf2_extractor.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tika import parser\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for extracting tika files\n",
    "class TikaExtract(object):\n",
    "    # initialize the object\n",
    "    def __init__(self, source_directory, target_directory_name):\n",
    "        # assigned variables for source_directory and target_directory_name\n",
    "        self.dir = source_directory\n",
    "        self.target = str(target_directory_name)\n",
    "    \n",
    "    # define recursive function to walk through directory and convert pdfs    \n",
    "    def extract_text_from_pdfs_recursively(self):\n",
    "        for root, dirs, files in os.walk(self.dir):\n",
    "            for file in files:\n",
    "                path_to_pdf = os.path.join(root, file)\n",
    "                [stem, ext] = os.path.splitext(path_to_pdf)\n",
    "                if ext == '.pdf':\n",
    "                    print(\"Processing \" + path_to_pdf)\n",
    "                    # use tika to parse contents from file\n",
    "                    pdf_contents = parser.from_file(path_to_pdf)\n",
    "                    # project specific - convert to raw\n",
    "                    raw_text = r'{}'.format(pdf_contents['content'])\n",
    "                    # project specific - replace new lines with spaces\n",
    "                    raw_text = raw_text.replace(\"\\n\",\" \")\n",
    "                    # project specific - replace double new lines with spaces\n",
    "                    raw_text = raw_text.replace(\"\\n\\n\" , \" \")\n",
    "                    # project specific - replace tabs with spaces\n",
    "                    raw_text = raw_text.replace(\"\\t\",\" \")\n",
    "                    path_to_txt = stem + '.txt'\n",
    "                    # check if target directory exists\n",
    "                    if not os.path.exists(str(os.getcwd()) + self.target):\n",
    "                        os.makedirs(str(os.getcwd()) + self.target)\n",
    "                    # write the text file to the target directory\n",
    "                    # names of the files will be the same, except have the .txt extension\n",
    "                    with open(str(os.getcwd()) + self.target + str(file[:-4]) + \".txt\", 'w') as txt_file:\n",
    "                        print(\"Writing contents to \" + str(os.getcwd()) + self.target + str(file[:-4]) + \".txt\")\n",
    "                        txt_file.write(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example, performing the operation on a local machine\n",
    "tikaextract = TikaExtract(source_directory='/Users/rahimjiwa/Documents/DataScience/UofT3666_AppliedNLP/Final_Testings/PDF_comparison',\n",
    "                         target_directory_name='/tika_PDF_comparison/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/rahimjiwa/Documents/DataScience/UofT3666_AppliedNLP/Final_Testings/PDF_comparison/3666_Module_9_Building_Chatbots.pdf\n",
      "Writing contents to /Users/rahimjiwa/Documents/DataScience/UofT3666_AppliedNLP/Final_Testings/PDF_comparison/tika_PDF_comparison/3666_Module_9_Building_Chatbots.txt\n"
     ]
    }
   ],
   "source": [
    "# run the function\n",
    "tikaextract.extract_text_from_pdfs_recursively()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Natural Language \n",
      "Processing\n",
      "\n",
      "Module 9: Building Chatbots\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Course Plan\n",
      "\n",
      "Module 1 – Introduction to Language Processing and Computation\n",
      "\n",
      "Module Titles\n",
      "\n",
      "Module 2 – Text Corpora & Pre-processing\n",
      "\n",
      "Module 3 – Introduction to Machine Learning\n",
      "\n",
      "Module 4 – Text Vectorization & Feature Engineering\n",
      "\n",
      "Module 5 – Applying Classification on Text\n",
      "\n",
      "Module 6 – Applying Clustering on Text\n",
      "\n",
      "Module 7 – Context Aware Language Modeling\n",
      "\n",
      "Module 8 – Text Visualization & Graph Analysis\n",
      "\n",
      "Module 9 – Building Chatbots\n",
      "\n",
      "Module 10 – Scaling with Multiprocessing and Spark\n",
      "\n",
      "Module 11 – Deep learning on Text data\n",
      "\n",
      "Module 12 – Team Project Presentations\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Learning Outcomes for this Module\n",
      "\n",
      "• We will\n",
      "\n",
      "learn a conversational\n",
      "\n",
      "chatbots, one of\n",
      "applications\n",
      "\n",
      "for building\n",
      "the fastest-growing language aware\n",
      "\n",
      "framework\n",
      "\n",
      "• We will demonstrate this framework by constructing a kitchen\n",
      "helper bot that can greet new users, perform measurement\n",
      "conversions, and recommend good recipes\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Topics for this Module\n",
      "\n",
      "9.1 Fundamentals of Conversational Agents\n",
      "\n",
      "9.2 Base Dialog system\n",
      "\n",
      "9.3 Rule based Conversation\n",
      "\n",
      "9.4 Question and Answer system\n",
      "\n",
      "9.5 Recommendation system\n",
      "\n",
      "9.6 Resources and Wrap-up\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Module 9 – Section 1\n",
      "\n",
      "Fundamentals of Conversational \n",
      "\n",
      "Agents\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Fundamentals of Conversation\n",
      "\n",
      "•\n",
      "\n",
      "In the 1940s, Claude Shannon and Warren Weaver, pioneers of\n",
      "information theory and machine translation, developed a model of\n",
      "communication.\n",
      "\n",
      "• TheShannon-Weaver model of communication is so influential\n",
      "\n",
      "that it is still used to understand conversation today\n",
      "\n",
      "•\n",
      "\n",
      "In their model, communication comes down to a series of\n",
      "encodings and transformations, as messages pass through\n",
      "channels with varying levels of noise and entropy, from initial\n",
      "source to destination.\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Fundamentals of Conversation\n",
      "\n",
      "• Modern notions of conversation, as shown in Figure 9-1, extend the\n",
      "\n",
      "Shannon–Weaver model, where\n",
      "\n",
      "– two (or more) parties take turns responding to each other’s messages.\n",
      "\n",
      "– Conversations take place over time and are generally bounded by a fixed\n",
      "\n",
      "length.\n",
      "\n",
      "– During a conversation, a participant can either be listening or speaking.\n",
      "Effective conversation requires at any given time a single speaker\n",
      "communicating and other participants listening.\n",
      "\n",
      "– Finally, the time-ordered record of the conversation must be consistent such\n",
      "in the\n",
      "\n",
      "that each statement makes sense given the previous statement\n",
      "conversation.\n",
      "\n",
      "Fig-9.1: Shannon-Weaver model of Conversation\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Fundamentals of Chatbots\n",
      "\n",
      "• A chatbot\n",
      "\n",
      "is a program that participates in turn-taking\n",
      "conversations and whose aim is to interpret input text or\n",
      "speech and to output appropriate, useful responses.\n",
      "\n",
      "• They require a computational means of grappling with the\n",
      "ambiguity of language and situational context in order to\n",
      "effectively parse incoming language and produce the most\n",
      "appropriate reply.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Fundamentals of Chatbots\n",
      "\n",
      "• A chatbot’s architecture, shown in Figure 9-2,\n",
      "\n",
      "is\n",
      "\n",
      "comprised of two primary components.\n",
      "– The first component\n",
      "\n",
      "is a user-facing interface that handles the\n",
      "mechanics of receiving user input (e.g., microphones or a web API)\n",
      "and delivering interpretable output (speakers or a mobile frontend).\n",
      "\n",
      "– This outer component wraps the second component, an internal dialog\n",
      "input, maintains an internal state, and\n",
      "\n",
      "interprets text\n",
      "\n",
      "system that\n",
      "produces responses.\n",
      "\n",
      "Fig-9.2: Architecture of a Chatbot\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Fundamentals of Chatbots\n",
      "\n",
      "•\n",
      "\n",
      "In this module we will focus on the internal dialog component\n",
      "and show how it can be easily generalized to any application\n",
      "and composed of multiple sub-dialogs.\n",
      "\n",
      "• To that end we will first create an abstract base class that\n",
      "formally defines the fundamental behavior or interface of the\n",
      "dialog.\n",
      "\n",
      "state management,\n",
      "\n",
      "• We will then explore three implementations of this base class\n",
      "for\n",
      "and\n",
      "recommendations and show how they can be composed as a\n",
      "single conversational agent.\n",
      "\n",
      "questions\n",
      "\n",
      "and\n",
      "\n",
      "answers,\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Module 9 – Section 2\n",
      "\n",
      "Base Dialog System\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Dialog system\n",
      "\n",
      "• A Dialog defines how we handle simple, brief exchanges and is\n",
      "the basic building block for conversational agents during an\n",
      "interaction between chatbot and user.\n",
      "\n",
      "• We will\n",
      "\n",
      "think of a conversation agent as composed of many\n",
      "\n",
      "internal dialogs that each handle their own area of responsibility\n",
      "\n",
      "• To ensure dialogs work together in concert, we must describe a\n",
      "\n",
      "single interface that defines how dialogs operate\n",
      "\n",
      "•\n",
      "\n",
      "In Python, we can use an abstract base class via the abc\n",
      "standard library module to list\n",
      "the methods and signatures\n",
      "expected of all subclasses. In this way we can ensure that all\n",
      "subclasses of our Dialog interface behave in an expected way\n",
      "\n",
      "• To create our interface, we’ll break this behavior into several\n",
      "\n",
      "methods that will be specifically defined in our subclasses\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Implementing Dialog module\n",
      "\n",
      "• The implementation starts with a non abstract method, listen, the\n",
      "primary entry point for a Dialog object that implements the general\n",
      "dialog behavior using abstract methods\n",
      "\n",
      "• The listen signature accepts text as a string, as well as a\n",
      "response boolean that indicates if initiative has passed to the\n",
      "Dialog and a response is required.\n",
      "\n",
      "•\n",
      "\n",
      "listen also takes arbitrary keyword arguments (kwargs) that may\n",
      "contain other contextual information such as the user, session id,\n",
      "or transcription score.\n",
      "\n",
      "• The output of this method is a response if required as well as a\n",
      "\n",
      "confidence score\n",
      "\n",
      "• Since we may not always be able to successfully parse and\n",
      "interpret incoming text, or formulate an appropriate response, this\n",
      "metric expresses a Dialog object’s confidence in its interpretation\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Implementing Dialog module\n",
      "\n",
      "• The parse method allows Dialog subclasses to implement their\n",
      "\n",
      "own mechanism for handling raw strings of data\n",
      "\n",
      "• The interpret method is responsible for\n",
      "\n",
      "interpreting an\n",
      "incoming list of parsed sentences, updating the internal state\n",
      "of\n",
      "the\n",
      "interpretation\n",
      "\n",
      "the Dialog, and computing a confidence level\n",
      "\n",
      "for\n",
      "\n",
      "• Finally, the respond method accepts interpreted sentences, a\n",
      "confidence score, and arbitrary keyword arguments in order to\n",
      "produce a text-based response based on the current state of\n",
      "the Dialog\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Implementing Dialog module\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Module 9 – Section 3\n",
      "\n",
      "Rule based Conversation \n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Rule based Greetings Module\n",
      "\n",
      "•\n",
      "\n",
      "In this section, we will implement a rules-based greeting feature\n",
      "inspired by these early models, which uses regular expressions to\n",
      "match utterances.\n",
      "\n",
      "• Our version will maintain state primarily to acknowledge\n",
      "participants entering and leaving the dialog, and respond to them\n",
      "with appropriate salutations and questions.\n",
      "\n",
      "• The Greeting dialog implements our conversational framework by\n",
      "\n",
      "extending the Dialog base class.\n",
      "\n",
      "• At the heart of the Greeting dialog is a dictionary, PATTERNS,\n",
      "stored as a class variable. This dictionary maps the kind of\n",
      "interactions (described by key) to a regular expression that defines\n",
      "the expected input for that interaction.\n",
      "In particular, our simple Greeting dialog is prepared for greetings,\n",
      "introductions, goodbyes, and roll calls\n",
      "\n",
      "•\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "Rule based Greetings module\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "Rule based Greetings module\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Rule based Greetings module\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Rule based Conversation module\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Module 9 – Section 4\n",
      "\n",
      "Question and Answer system\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "Building Question and Answer System\n",
      "\n",
      "• One of the most common uses of chatbots is to quickly and easily\n",
      "answer fact-based questions such as “How long is the Nile river?”\n",
      "\n",
      "• There exists a variety of fact and knowledge bases on the web\n",
      "\n",
      "such as DBPedia, Yago2, and Google Knowledge Graph\n",
      "\n",
      "• The challenge is converting a natural\n",
      "\n",
      "language question into a\n",
      "database query. While statistical matching of questions to their\n",
      "answers is one simple mechanism for\n",
      "this, more robust\n",
      "approaches use both statistical and semantic information;\n",
      "\n",
      "•\n",
      "\n",
      "for example, using a frame-based approach to create templates\n",
      "that can be derived into SPARQLqueries or using classification\n",
      "techniques to identify the type of answer required (e.g., a location,\n",
      "an amount, etc.)\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "Detecting Questions\n",
      "\n",
      "• The first step of building this system is to detect when we’ve been\n",
      "\n",
      "asked a question, and to determine what type of question it is.\n",
      "\n",
      "• An excellent first step is to consider what questions look like\n",
      "\n",
      "• Though questions are often posed in irregular and unexpected\n",
      "\n",
      "ways, some patterns do exist.\n",
      "\n",
      "• To detect these patterns, we will need to perform some type of\n",
      "syntactic parsing—in other words, a mechanism that exploits\n",
      "context-free grammars to systematically assign syntactic structure to\n",
      "incoming text\n",
      "\n",
      "• We have seen that NLTK has a number of grammer-based parsers,\n",
      "but all require us to provide a grammar to specify the rules, which\n",
      "will unnecessarily limit our chatbot’s flexibility\n",
      "\n",
      "• Next, we will\n",
      "\n",
      "instead explore pretrained parsing model as more\n",
      "\n",
      "flexible alternatives:\n",
      "– dependency parsing\n",
      "\n",
      "– constituency parsing\n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "Dependency Parsing\n",
      "\n",
      "• Dependency parsers are a lightweight mechanism to extract the\n",
      "syntactic structure of a sentence by linking phrases together with\n",
      "specific relationships.\n",
      "\n",
      "• They do so by first identifying the head word of a phrase, then\n",
      "\n",
      "establishing links between the words that modify the head.\n",
      "\n",
      "• The result\n",
      "\n",
      "is an overlapping structure of arcs that\n",
      "\n",
      "identify\n",
      "\n",
      "meaningful substructures of the sentence.\n",
      "\n",
      "• Consider\n",
      "\n",
      "the sentence “How many teaspoons are in a\n",
      "In Figure 9-3, we see a dependency parse as\n",
      "\n",
      "tablespoon?”.\n",
      "visualized using SpaCy’s DisplaCy module\n",
      "\n",
      "Fig-9.3: Spacy Dependency Tree\n",
      "\n",
      "25\n",
      "\n",
      "\f",
      "Implementing Dependency Parsing\n",
      "\n",
      "• To recreate the parse shown in Figure 9-3, we first load SpaCy’s\n",
      "\n",
      "prebuilt English language parsing model.\n",
      "\n",
      "• We then write a function, plot_displacy_tree, that parses incoming\n",
      "sentences using the prebuilt model and plots the resulting\n",
      "dependency parse using the displacy.serve method.\n",
      "\n",
      "26\n",
      "\n",
      "\f",
      "Problem with Dependency Parsing\n",
      "\n",
      "• Dependency parsers are extremely popular for producing fast and\n",
      "correct grammatical analyses and when combined with part-of-\n",
      "speech tagging, perform much of the work required for phrase-\n",
      "level analysis.\n",
      "\n",
      "• However, dependency parsing does not offer as rich and deep a\n",
      "view of the structure of sentences, and as such, may not always\n",
      "be sufficient or optimal.\n",
      "\n",
      "•\n",
      "\n",
      "In our chatbot, we’ll demonstrate how to leverage a more\n",
      "comprehensive tree representation, the constituency parsing.\n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Constituency Parsing\n",
      "\n",
      "• Constituency parsing is a form of syntactic parsing whose goal is\n",
      "\n",
      "to break down a sentence into nested phrase structures\n",
      "\n",
      "• The output\n",
      "\n",
      "is a tree structure that\n",
      "\n",
      "captures\n",
      "\n",
      "complex\n",
      "\n",
      "interrelationships between sub-phrases.\n",
      "\n",
      "• Constituency parsers provide an opportunity to apply tree-\n",
      "\n",
      "traversal algorithms that easily enable computation on text\n",
      "\n",
      "• Here we can see a much more complex structure of sub-phrases\n",
      "and more direct, unlabeled relationships between nodes in the\n",
      "tree.\n",
      "\n",
      "28\n",
      "\n",
      "\f",
      "Constituency Parsing\n",
      "\n",
      "• Constituency parse trees are comprised of terminal\n",
      "\n",
      "leaf nodes,\n",
      "\n",
      "the part-of-speech tag, and the word itself\n",
      "\n",
      "• The nonterminal nodes represent phrases that join the part-of-\n",
      "\n",
      "speech tags into related groupings\n",
      "\n",
      "•\n",
      "\n",
      "In our example constituency tree, the root phrase is an SBARQ, a\n",
      "clause identified as a direct question because it is introduced by a\n",
      "“wh”-word\n",
      "\n",
      "Fig 9.4: Constituency parsing of “How many teaspoons are in a \n",
      "tablespoon”\n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "Implementing Constituency Parsing\n",
      "\n",
      "• The Stanford CoreNLP package can be used to execute constituency\n",
      "\n",
      "parsing\n",
      "\n",
      "• NLTK made available a new module nltk.parse.stanford that enables us\n",
      "to use the Stanford parsers from inside NLTK (assuming you have set up\n",
      "the requisite .jars and PATH configuration) as follows:\n",
      "\n",
      "30\n",
      "\n",
      "\f",
      "Identifying Questions\n",
      "\n",
      "• The pretrained models in SpaCy and CoreNLP give us a powerful\n",
      "\n",
      "way to automatically parse and annotate input sentences.\n",
      "\n",
      "• We can then use the annotations to traverse the parsed\n",
      "sentences and look for part-of-speech tags that correspond to\n",
      "questions.\n",
      "\n",
      "• We can explore how different questions manifest using the Penn\n",
      "Treebank tags, which we first encountered in “Part-of-Speech\n",
      "Tagging” in module-2\n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "Identifying Questions\n",
      "\n",
      "•\n",
      "\n",
      "In our example, we can see from the root that our input is an\n",
      "SBARQ (a direct question introduced by a “wh”-word), which in\n",
      "this case is a WRB (a “wh”-adverb).\n",
      "\n",
      "32\n",
      "\n",
      "\f",
      "Identifying Questions\n",
      "\n",
      "• The major advantage of using a technique like this for question detection\n",
      "\n",
      "is the flexibility.\n",
      "\n",
      "• For instance, if we change our question to “Sorry to trouble you, but how\n",
      "many teaspoons are in a tablespoon?”, the output is different, but the\n",
      "WHADJP and WRB question markers are still there\n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "Penn Treebank II Tags for Question Detection\n",
      "\n",
      "34\n",
      "\n",
      "\f",
      "Implementing Question-Answer Module\n",
      "\n",
      "• Now we will add to our chatbot is a question-and-answer system that\n",
      "leverages the pretrained parsers to provide convenient kitchen\n",
      "measurement conversions (questions about measurements as “How”\n",
      "questions).\n",
      "\n",
      "• For our question-type identification task, we will aim to be able to\n",
      "\n",
      "interpret questions that take the form “How many X are in a Y?”\n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "Implemention – Converter class\n",
      "\n",
      "• The main class we write is Converter, which inherits the behavior of our\n",
      "\n",
      "Dialog class.\n",
      "\n",
      "• We need to initialize a Converter with a knowledge base of measurement\n",
      "conversions, here a simple JSON file stored in CONVERSION_PATH and\n",
      "containing all of the conversions between units of measure.\n",
      "\n",
      "• We also initialize a parser (here we use CoreNLP), as well as a stemmer\n",
      "from NLTK and an inflect.engine from the inflect library, which will enable us\n",
      "to handle pluralization in the parse and respond methods, respectively.\n",
      "\n",
      "• Our parse method will use the raw_parse method from CoreNLP to\n",
      "\n",
      "generate constituency parses as demonstrated in the previous section\n",
      "\n",
      "36\n",
      "\n",
      "\f",
      "Implemention- Interpret method\n",
      "\n",
      "• Next,\n",
      "\n",
      "to collect\n",
      "\n",
      "in interpret method, we initialize a list\n",
      "\n",
      "the\n",
      "measures we want to convert from and to, an initial confidence\n",
      "score of 0, and a dictionary to collect\n",
      "the results of our\n",
      "interpretation\n",
      "\n",
      "• We retrieve the root of\n",
      "\n",
      "the parsed sentence tree and scan\n",
      "through the part-of-speech tags for ones that match the adverbial\n",
      "phrase question pattern (WRB).\n",
      "\n",
      "•\n",
      "\n",
      "If we identify any numbers (Tag CD: cardinal number) within the\n",
      "question phrase subtree, we store that in our results dictionary as\n",
      "the quantity for the target measure.\n",
      "\n",
      "• Finally, we return a (results, confidence, kwargs) tuple, which the\n",
      "respond method will use to determine whether and how to\n",
      "respond to the user\n",
      "\n",
      "37\n",
      "\n",
      "\f",
      "Implemention- Interpret method\n",
      "\n",
      "38\n",
      "\n",
      "\f",
      "Implementation- Respond Method\n",
      "\n",
      "• Before implementing our respond method, we need a few helper\n",
      "\n",
      "utilities.\n",
      "\n",
      "• The first\n",
      "\n",
      "is convert, which converts from the units of\n",
      "\n",
      "the source\n",
      "\n",
      "measurement to those of the target measurement.\n",
      "\n",
      "• We also need to add round, pluralize, and numericalize methods,\n",
      "which leverage utilities from the humanize library to transform\n",
      "numbers to more natural human-readable form\n",
      "\n",
      "• Finally, in the respond method, we check to see if our confidence in\n",
      "our interpretation is sufficiently high, and if so, we use convert to\n",
      "perform the actual measurement conversions, and then round,\n",
      "pluralize, and numericalize to ensure the final response is easy for the\n",
      "user to read\n",
      "\n",
      "39\n",
      "\n",
      "\f",
      "Implementation- Respond Method\n",
      "\n",
      "40\n",
      "\n",
      "\f",
      "Implementation- Respond Method\n",
      "\n",
      "41\n",
      "\n",
      "\f",
      "Implementation- Listen Method\n",
      "\n",
      "• Now we can experiment with using the listen method on a few\n",
      "possible input questions to see how well our Converter class is able\n",
      "to handle different combinations and quantities of source and target\n",
      "units.\n",
      "\n",
      "42\n",
      "\n",
      "\f",
      "Module 9 – Section 5\n",
      "\n",
      "Recommender system\n",
      "\n",
      "43\n",
      "\n",
      "\f",
      "Building Recipe Recommender module\n",
      "\n",
      "•\n",
      "\n",
      "In this section we will be adding a module to our chatbot – a\n",
      "recipe recommender.\n",
      "\n",
      "• As shown in the pipeline, there will be two phase of this module:\n",
      "\n",
      "– In Build phase, we need to train a recommender system through using a\n",
      "\n",
      "classification algorithm and a cooking corpus\n",
      "\n",
      "– In Deploy phase, it will suggest a recipe based on the ingredients during\n",
      "\n",
      "conversation with a user.\n",
      "\n",
      "Fig-9.5: Recipe Recommender Schema\n",
      "\n",
      "44\n",
      "\n",
      "\f",
      "Building Recommender model\n",
      "\n",
      "• To train our\n",
      "\n",
      "recommender, we’ll be using a cooking corpus\n",
      "comprised of blog posts and articles that contain recipes for specific\n",
      "dishes as well as narratives and descriptions for those dishes.\n",
      "\n",
      "• We will build the pipeline that will leverage a recipe corpus to perform\n",
      "text normalization, vectorization, and dimensionality reduction, and\n",
      "finally use a nearest-neighbor algorithm to provide recipe\n",
      "recommendations.\n",
      "\n",
      "• Usually the main challenges of using nearest-neighbor algorithms is\n",
      "the complexity of search increases as dimensionality increases;\n",
      "Hence, in case of text applications, dimensionality reduction is often\n",
      "an important step.\n",
      "\n",
      "• Also we will leverage a less computationally expensive alternative to\n",
      "tree\n",
      "\n",
      "traditional unsupervised nearest-neighbor search – Ball\n",
      "algorithm.\n",
      "\n",
      "45\n",
      "\n",
      "\f",
      "Implementation- BallTreerecommender\n",
      "\n",
      "• First, we’ll create a BallTreeRecommender class, which is\n",
      "initialized with a k for the desired number of recommendations\n",
      "(which will default to 3), paths to a pickled fitted transformer\n",
      "svd.pkl, and fitted ball tree tree.pkl.\n",
      "If the model has already been fit, these paths will exist, and the\n",
      "load method will load them from disk for use.\n",
      "\n",
      "•\n",
      "\n",
      "• Otherwise, they will be fitted and saved using ScikitLearn’s\n",
      "\n",
      "joblib serializer in our fit_transform method.\n",
      "\n",
      "• Once a sklearn.neighbors.BallTree model has been fitted, we\n",
      "can use the tree.query method that uses the fitted transformer\n",
      "to vectorize and transform incoming text and return only the\n",
      "indices for the closest recipes.\n",
      "\n",
      "46\n",
      "\n",
      "\f",
      "Implementation- BallTreerecommender\n",
      "\n",
      "47\n",
      "\n",
      "\f",
      "Implementation- BallTreerecommender\n",
      "\n",
      "48\n",
      "\n",
      "\f",
      "Recommending recipe during chat\n",
      "\n",
      "• Once we have fit our BallTreeRecommender on our pickled recipe\n",
      "corpus and saved the model artifacts, we can now implement the\n",
      "recipe recommendations in the context of our Dialog abstract\n",
      "base class.\n",
      "\n",
      "• We will be reusing “HTML PickledCorpusReader” class we\n",
      "defined earlier, except that we will be adding a titles() method that\n",
      "will allow us to grab the page titles from each HTMLfile, which will\n",
      "give us a human-intelligible way to refer to each recipe.\n",
      "\n",
      "49\n",
      "\n",
      "\f",
      "Recommending recipe during chat\n",
      "\n",
      "• Our new class RecipeRecommender is instantiated with a pickled\n",
      "implements\n",
      "\n",
      "estimator\n",
      "\n",
      "that\n",
      "\n",
      "an\n",
      "\n",
      "corpus\n",
      "BallTreeRecommender model.\n",
      "\n",
      "reader\n",
      "\n",
      "and\n",
      "\n",
      "•\n",
      "\n",
      "the recommender isn’t already fitted,\n",
      "\n",
      "If\n",
      "ensure that it is fit and transformed.\n",
      "\n",
      "the __init__ method will\n",
      "\n",
      "• Next, the parse method splits the input text string into a list and\n",
      "\n",
      "performs part-of-speech tagging.\n",
      "\n",
      "• The interpret method takes in the parsed text and determines\n",
      "whether it is a list of ingredients. If so, it transforms the utterance into\n",
      "a collection of nouns and then assigns a confidence score according\n",
      "to the percent of the input text that is nouns.\n",
      "\n",
      "• Finally, the respond method takes in the list of nouns extracted from\n",
      "the interpret method as well as the confidence.\n",
      "interpret has\n",
      "successfully extracted a sufficient number of nouns from the input,\n",
      "the confidence will be high enough to generate recommendations.\n",
      "\n",
      "If\n",
      "\n",
      "50\n",
      "\n",
      "\f",
      "Recommending recipe during chat\n",
      "\n",
      "51\n",
      "\n",
      "\f",
      "Recommending recipe during chat\n",
      "\n",
      "One Sample Result:\n",
      "\n",
      "52\n",
      "\n",
      "\f",
      "Module 9 – Section 5\n",
      "\n",
      "Resources and Wrap-up\n",
      "\n",
      "53\n",
      "\n",
      "\f",
      "Homework\n",
      "\n",
      "• Complete the notebook in the assignments section for this \n",
      "\n",
      "week\n",
      "\n",
      "54\n",
      "\n",
      "\f",
      "Next Class \n",
      "\n",
      "• Scaling with Multiprocessing and Spark\n",
      "\n",
      "• Reading:  Chapter 11 textbook\n",
      "\n",
      "55\n",
      "\n",
      "\f",
      "Follow us on social\n",
      "\n",
      "Join the conversation with us online:\n",
      "\n",
      "facebook.com/uoftscs\n",
      "\n",
      "@uoftscs\n",
      "\n",
      "linkedin.com/company/university-of-toronto-school-of-continuing-studies\n",
      "\n",
      "@uoftscs\n",
      "\n",
      "56\n",
      "\n",
      "\f",
      "Any questions?\n",
      "\n",
      "57\n",
      "\n",
      "\f",
      "Thank You\n",
      "\n",
      "Thank you for choosing the University of Toronto \n",
      "School of Continuing Studies\n",
      "\n",
      "58\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('3666_Module_9_Building_Chatbots.txt','r') as f:\n",
    "    pdf_miner_output = f.read()\n",
    "print(pdf_miner_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 Applied Natural Language  Processing Module 9: Building  Chatbots ', '2 Module Titles Module  1   Introduction to Language Processing and Computation Module 2   Text Corpora & Pre - processing Module 3   Introduction to Machine Learning Module 4   Text Vectorization & Feature Engineering Module 5   Applying Classification on Text Module 6   Applying Clustering on Text Module 7   Context Aware Language Modeling Module  8   Text Visualization & Graph Analysis Module  9   Building Chatbots Module  10   Scaling with Multiprocessing and Spark Module  11   Deep learning on Text data Module  12   Team Project Presentations Course Plan ', '3  We will learn a conversational framework for building chatbots , one of the fastest - growing language aware applications  We will demonstrate this framework by constructing a kitchen helper bot that can greet new users, perform measurement conversions, and recommend good recipes Learning Outcomes for this Module ', '4 9 .1 Fundamentals of Conversational Agents 9.2 Base Dialog  system 9.3 Rule based Conversation 9.4 Question and Answer system 9.5 Recommendation system 9.6 Resources and Wrap - up Topics for this Module ', '5 Module  9   Section 1 Fundamentals of  Conversational  Agents ', '6 Fundamentals of Conversation  In the 1940 s, Claude Shannon and Warren Weaver, pioneers of information theory and machine translation, developed a model of communication .  TheShannon - Weaver model of communication is so influential that it is still used to understand conversation today  In their model, communication comes down to a series of encodings and transformations, as messages pass through channels with varying levels of noise and entropy, from initial source to destination . ', '7 Fundamentals of Conversation  Modern notions of conversation, as shown in Figure 9 - 1 , extend the Shannon  Weaver model, where  two (or more) parties take turns responding to each  messages .  Conversations take place over time and are generally bounded by a fixed length .  During a conversation, a participant can either be listening or speaking . Effective conversation requires at any given time a single speaker communicating and other participants listening .  Finally , the time - ordered record of the conversation must be consistent such that each statement makes sense given the previous statement in the conversation . Fig - 9.1: Shannon - Weaver model of Conversation ', '8 Fundamentals of  Chatbots  A chatbot is a program that participates in turn - taking conversations and whose aim is to interpret input text or speech and to output appropriate, useful responses .  They require a computational means of grappling with the ambiguity of language and situational context in order to effectively parse incoming language and produce the most appropriate reply . ', '9 Fundamentals of  Chatbots  A  architecture, shown in Figure 9 - 2 , is comprised of two primary components .  The first component is a user - facing interface that handles the mechanics of receiving user input (e . g . , microphones or a web API) and delivering interpretable output (speakers or a mobile frontend ) .  This outer component wraps the second component, an internal dialog system that interprets text input, maintains an internal state, and produces responses . Fig - 9.2: Architecture of a  Chatbot ', '10 Fundamentals of  Chatbots  In this module we will focus on the internal dialog component and show how it can be easily generalized to any application and composed of multiple sub - dialogs .  To that end we will first create an abstract base class that formally defines the fundamental behavior or interface of the dialog .  We will then explore three implementations of this base class for state management, questions and answers, and recommendations and show how they can be composed as a single conversational agent . ', '11 Module  9   Section  2 Base Dialog System ', '12 Dialog system  A Dialog defines how we handle simple, brief exchanges and is the basic building block for conversational agents during an interaction between chatbot and user .  We will think of a conversation agent as composed of many internal dialogs that each handle their own area of responsibility  To ensure dialogs work together in concert, we must describe a single interface that defines how dialogs operate  In Python, we can use an abstract base class via the abc standard library module to list the methods and signatures expected of all subclasses . In this way we can ensure that all subclasses of our Dialog interface behave in an expected way  To create our interface,  break this behavior into several methods that will be specifically defined in our subclasses ', '13 Implementing Dialog module  The implementation starts with a non abstract method, listen , the primary entry point for a Dialog object that implements the general dialog behavior using abstract methods  The listen signature accepts text as a string, as well as a response boolean that indicates if initiative has passed to the Dialog and a response is required .  listen also takes arbitrary keyword arguments ( kwargs ) that may contain other contextual information such as the user, session id, or transcription score .  The output of this method is a response if required as well as a confidence score  Since we may not always be able to successfully parse and interpret incoming text, or formulate an appropriate response, this metric expresses a Dialog  confidence in its interpretation ', '14 Implementing Dialog module  The parse method allows Dialog subclasses to implement their own mechanism for handling raw strings of data  The interpret method is responsible for interpreting an incoming list of parsed sentences, updating the internal state of the Dialog, and computing a confidence level for the interpretation  Finally, the respond method accepts interpreted sentences, a confidence score, and arbitrary keyword arguments in order to produce a text - based response based on the current state of the Dialog ', '15 Implementing Dialog module ', '16 Module  9   Section  3 Rule based Conversation  ', '17 Rule based Greetings Module  In this section, we will implement a rules - based greeting feature inspired by these early models, which uses regular expressions to match utterances .  Our version will maintain state primarily to acknowledge participants entering and leaving the dialog, and respond to them with appropriate salutations and questions .  The Greeting dialog implements our conversational framework by extending the Dialog base class .  At the heart of the Greeting dialog is a dictionary, PATTERNS, stored as a class variable . This dictionary maps the kind of interactions (described by key) to a regular expression that defines the expected input for that interaction .  In particular, our simple Greeting dialog is prepared for greetings, introductions, goodbyes, and roll calls ', '18 Rule based Greetings module ', '19 Rule based Greetings module ', '20 Rule based Greetings module ', '21 Rule based Conversation module ', '22 Module  9   Section  4 Question and Answer system ', '23 Building Question and Answer System  One of the most common uses of chatbots is to quickly and easily answer fact - based questions such as  long is the Nile   There exists a variety of fact and knowledge bases on the web such as DBPedia , Yago 2 , and Google Knowledge Graph  The challenge is converting a natural language question into a database query . While statistical matching of questions to their answers is one simple mechanism for this, more robust approaches use both statistical and semantic information ;  for example, using a frame - based approach to create templates that can be derived into SPARQLqueries or using classification techniques to identify the type of answer required (e . g . , a location, an amount, etc . ) ', '24 Detecting Questions  The first step of building this system is to detect when  been asked a question, and to determine what type of question it is .  An excellent first step is to consider what questions look like  Though questions are often posed in irregular and unexpected ways, some patterns do exist .  To detect these patterns, we will need to perform some type of syntactic parsing  in other words, a mechanism that exploits context - free grammars to systematically assign syntactic structure to incoming text  We have seen that NLTK has a number of grammer - based parsers, but all require us to provide a grammar to specify the rules, which will unnecessarily limit our  flexibility  Next, we will instead explore pretrained parsing model as more flexible alternatives :  dependency parsing  constituency parsing ', '25 Dependency Parsing  Dependency parsers are a lightweight mechanism to extract the syntactic structure of a sentence by linking phrases together with specific relationships .  They do so by first identifying the head word of a phrase, then establishing links between the words that modify the head .  The result is an overlapping structure of arcs that identify meaningful substructures of the sentence .  Consider the sentence  many teaspoons are in a  . In Figure 9 - 3 , we see a dependency parse as visualized using  DisplaCy module Fig - 9.3: Spacy Dependency Tree ', '26 Implementing Dependency  P arsing  To recreate the parse shown in Figure 9 - 3 , we first load  prebuilt English language parsing model .  We then write a function, plot_displacy_tree , that parses incoming sentences using the prebuilt model and plots the resulting dependency parse using the displacy . serve method . ', '27 Problem with Dependency  P arsing  Dependency parsers are extremely popular for producing fast and correct grammatical analyses and when combined with part - of - speech tagging, perform much of the work required for phrase - level analysis .  However , dependency parsing does not offer as rich and deep a view of the structure of sentences, and as such, may not always be sufficient or optimal .  In our chatbot ,  demonstrate how to leverage a more comprehensive tree representation, the constituency parsing . ', '28 Constituency  P arsing  Constituency parsing is a form of syntactic parsing whose goal is to break down a sentence into nested phrase structures  The output is a tree structure that captures complex interrelationships between sub - phrases .  Constituency parsers provide an opportunity to apply tree - traversal algorithms that easily enable computation on text  Here we can see a much more complex structure of sub - phrases and more direct, unlabeled relationships between nodes in the tree . ', '29 Constituency  P arsing  Constituency parse trees are comprised of terminal leaf nodes, the part - of - speech tag, and the word itself  The nonterminal nodes represent phrases that join the part - of - speech tags into related groupings  In our example constituency tree, the root phrase is an SBARQ, a clause identified as a direct question because it is introduced by a  wh  - word   ', '30 Implementing Constituency  P arsing  The Stanford CoreNLP package can be used to execute constituency parsing  NLTK made available a new module nltk . parse . stanford that enables us to use the Stanford parsers from inside NLTK (assuming you have set up the requisite . jars and PATH configuration) as follows : ', '31 Identifying Questions  The pretrained models in SpaCy and CoreNLP give us a powerful way to automatically parse and annotate input sentences .  We can then use the annotations to traverse the parsed sentences and look for part - of - speech tags that correspond to questions .  We can explore how different questions manifest using the Penn Treebank tags, which we first encountered in  - of - Speech Tagging  in module - 2 ', '32 Identifying Questions  In our example, we can see from the root that our input is an SBARQ (a direct question introduced by a  wh  - word), which in this case is a WRB (a  wh  - adverb ) . ', '33 Identifying Questions  The major advantage of using a technique like this for question detection is the flexibility .  For instance, if we change our question to  to trouble you, but how many teaspoons are in a  the output is different, but the WHADJP and WRB question markers are still there ', '34 Penn Treebank II Tags for Question Detection ', '35 Implementing Question - Answer Module  Now we will add to our chatbot is a question - and - answer system that leverages the pretrained parsers to provide convenient kitchen measurement conversions ( questions about measurements as  questions ) .  For our question - type identification task, we will aim to be able to interpret questions that take the form  many X are in a  ', '36 Implemention  Converter class  The main class we write is Converter , which inherits the behavior of our Dialog class .  We need to initialize a Converter with a knowledge base of measurement conversions, here a simple JSON file stored in CONVERSION_PATH and containing all of the conversions between units of measure .  We also initialize a parser (here we use CoreNLP ), as well as a stemmer from NLTK and an inflect . engine from the inflect library, which will enable us to handle pluralization in the parse and respond methods, respectively .  Our parse method will use the raw_parse method from CoreNLP to generate constituency parses as demonstrated in the previous section ', '37 Implemention - Interpret method  Next , in interpret method, we initialize a list to collect the measures we want to convert from and to, an initial confidence score of 0 , and a dictionary to collect the results of our interpretation  We retrieve the root of the parsed sentence tree and scan through the part - of - speech tags for ones that match the adverbial phrase question pattern (WRB ) .  If we identify any numbers (Tag CD : cardinal number) within the question phrase subtree, we store that in our results dictionary as the quantity for the target measure .  Finally, we return a (results, confidence, kwargs ) tuple, which the respond method will use to determine whether and how to respond to the user ', '38 Implemention - Interpret method ', '39 Implementation - Respond Method  B efore implementing our respond method, we need a few helper utilities .  The first is convert , which converts from the units of the source measurement to those of the target measurement .  We also need to add round , pluralize , and numericalize methods, which leverage utilities from the humanize library to transform numbers to more natural human - readable form  Finally, in the respond method, we check to see if our confidence in our interpretation is sufficiently high, and if so, we use convert to perform the actual measurement conversions, and then round, pluralize, and numericalize to ensure the final response is easy for the user to read ', '40 Implementation - Respond Method ', '41 Implementation - Respond Method ', '42 Implementation - Listen Method  Now we can experiment with using the listen method on a few possible input questions to see how well our Converter class is able to handle different combinations and quantities of source and target units . ', '43 Module  9   Section  5 Recommender system ', '44 Building Recipe Recommender module  In this section we will be adding a module to our chatbot  a recipe recommender .  As shown in the pipeline, there will be two phase of this module :  In Build phase, we need to train a recommender system through using a classification algorithm and a cooking corpus  In Deploy phase, it will suggest a recipe based on the ingredients during conversation with a user . Fig - 9.5: Recipe Recommender Schema ', '45 Building Recommender model  To train our recommender,  be using a cooking corpus comprised of blog posts and articles that contain recipes for specific dishes as well as narratives and descriptions for those dishes .  We will build the pipeline that will leverage a recipe corpus to perform text normalization, vectorization, and dimensionality reduction, and finally use a nearest - neighbor algorithm to provide recipe recommendations .  Usually the main challenges of using nearest - neighbor algorithms is the complexity of search increases as dimensionality increases ; Hence, in case of text applications, dimensionality reduction is often an important step .  Also we will leverage a less computationally expensive alternative to traditional unsupervised nearest - neighbor search  Ball tree algorithm . ', '46 Implementation - BallTreerecommender  First,  create a BallTreeRecommender class, which is initialized with a k for the desired number of recommendations (which will default to 3 ), paths to a pickled fitted transformer svd . pkl , and fitted ball tree tree . pkl .  If the model has already been fit, these paths will exist, and the load method will load them from disk for use .  Otherwise , they will be fitted and saved using  joblib serializer in our fit_transform method .  Once a sklearn . neighbors . BallTree model has been fitted, we can use the tree . query method that uses the fitted transformer to vectorize and transform incoming text and return only the indices for the closest recipes . ', '47 Implementation - BallTreerecommender ', '48 Implementation - BallTreerecommender ', '49 Recommending recipe during chat  Once we have fit our BallTreeRecommender on our pickled recipe corpus and saved the model artifacts, we can now implement the recipe recommendations in the context of our Dialog abstract base class .  We will be reusing  PickledCorpusReader  class we defined earlier, except that we will be adding a titles() method that will allow us to grab the page titles from each HTMLfile , which will give us a human - intelligible way to refer to each recipe . ', '50 Recommending recipe during chat  Our new class RecipeRecommender is instantiated with a pickled corpus reader and an estimator that implements BallTreeRecommender model .  If the recommender  already fitted, the __ init __ method will ensure that it is fit and transformed .  Next, the parse method splits the input text string into a list and performs part - of - speech tagging .  The interpret method takes in the parsed text and determines whether it is a list of ingredients . If so, it transforms the utterance into a collection of nouns and then assigns a confidence score according to the percent of the input text that is nouns .  Finally, the respond method takes in the list of nouns extracted from the interpret method as well as the confidence . If interpret has successfully extracted a sufficient number of nouns from the input, the confidence will be high enough to generate recommendations . ', '51 Recommending recipe during chat ', '52 Recommending recipe during chat One Sample Result: ', '53 Module  9   Section  5 Resources and Wrap - up ', '54  Complete the notebook in the assignments section for this  week Homework ', '55  Scaling with Multiprocessing and Spark  Reading:  Chapter  11  textbook Next Class  ', '56 Join the conversation with us online: facebook.com/ uoftscs @ uoftscs linkedin.com /company/university - of - toronto - school - of - continuing - studies @ uoftscs Follow us on social ', '57 Any questions? ', '58 Thank you for choosing the University of Toronto  School of Continuing Studies Thank You ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.getcwd() +'/PDF_comparison/3666_Module_9_Building_Chatbots.txt') as f:\n",
    "    pypdf2_output = f.read()\n",
    "print(pypdf2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           PowerPoint Presentation   1  Applied Natural Language   Processing  Module 9: Building Chatbots    2  Module Titles  Module 1 – Introduction to Language Processing and Computation  Module 2 – Text Corpora & Pre-processing  Module 3 – Introduction to Machine Learning  Module 4 – Text Vectorization & Feature Engineering  Module 5 – Applying Classification on Text  Module 6 – Applying Clustering on Text  Module 7 – Context Aware Language Modeling  Module 8 – Text Visualization & Graph Analysis  Module 9 – Building Chatbots  Module 10 – Scaling with Multiprocessing and Spark  Module 11 – Deep learning on Text data  Module 12 – Team Project Presentations  Course Plan    3  • We will learn a conversational framework for building  chatbots, one of the fastest-growing language aware  applications  • We will demonstrate this framework by constructing a kitchen  helper bot that can greet new users, perform measurement  conversions, and recommend good recipes  Learning Outcomes for this Module    4  9.1 Fundamentals of Conversational Agents  9.2 Base Dialog system  9.3 Rule based Conversation  9.4 Question and Answer system  9.5 Recommendation system  9.6 Resources and Wrap-up  Topics for this Module    5  Module 9 – Section 1  Fundamentals of Conversational   Agents    6  Fundamentals of Conversation  • In the 1940s, Claude Shannon and Warren Weaver, pioneers of  information theory and machine translation, developed a model of  communication.  • TheShannon-Weaver model of communication is so influential  that it is still used to understand conversation today  • In their model, communication comes down to a series of  encodings and transformations, as messages pass through  channels with varying levels of noise and entropy, from initial  source to destination.    7  Fundamentals of Conversation  • Modern notions of conversation, as shown in Figure 9-1, extend the  Shannon–Weaver model, where  – two (or more) parties take turns responding to each other’s messages.  – Conversations take place over time and are generally bounded by a fixed  length.  – During a conversation, a participant can either be listening or speaking.  Effective conversation requires at any given time a single speaker  communicating and other participants listening.  – Finally, the time-ordered record of the conversation must be consistent such  that each statement makes sense given the previous statement in the  conversation.  Fig-9.1: Shannon-Weaver model of Conversation    8  Fundamentals of Chatbots  • A chatbot is a program that participates in turn-taking  conversations and whose aim is to interpret input text or  speech and to output appropriate, useful responses.  • They require a computational means of grappling with the  ambiguity of language and situational context in order to  effectively parse incoming language and produce the most  appropriate reply.    9  Fundamentals of Chatbots  • A chatbot’s architecture, shown in Figure 9-2, is  comprised of two primary components. – The first component is a user-facing interface that handles the  mechanics of receiving user input (e.g., microphones or a web API)  and delivering interpretable output (speakers or a mobile frontend).  – This outer component wraps the second component, an internal dialog  system that interprets text input, maintains an internal state, and  produces responses.  Fig-9.2: Architecture of a Chatbot    10  Fundamentals of Chatbots  • In this module we will focus on the internal dialog component  and show how it can be easily generalized to any application  and composed of multiple sub-dialogs.  • To that end we will first create an abstract base class that  formally defines the fundamental behavior or interface of the  dialog.  • We will then explore three implementations of this base class  for state management, questions and answers, and  recommendations and show how they can be composed as a  single conversational agent.    11  Module 9 – Section 2  Base Dialog System    12  Dialog system  • A Dialog defines how we handle simple, brief exchanges and is  the basic building block for conversational agents during an  interaction between chatbot and user.  • We will think of a conversation agent as composed of many  internal dialogs that each handle their own area of responsibility  • To ensure dialogs work together in concert, we must describe a  single interface that defines how dialogs operate  • In Python, we can use an abstract base class via the abc  standard library module to list the methods and signatures  expected of all subclasses. In this way we can ensure that all  subclasses of our Dialog interface behave in an expected way  • To create our interface, we’ll break this behavior into several  methods that will be specifically defined in our subclasses    13  Implementing Dialog module  • The implementation starts with a non abstract method, listen, the  primary entry point for a Dialog object that implements the general  dialog behavior using abstract methods  • The listen signature accepts text as a string, as well as a  response boolean that indicates if initiative has passed to the  Dialog and a response is required.  • listen also takes arbitrary keyword arguments (kwargs) that may  contain other contextual information such as the user, session id,  or transcription score.  • The output of this method is a response if required as well as a  confidence score  • Since we may not always be able to successfully parse and  interpret incoming text, or formulate an appropriate response, this  metric expresses a Dialog object’s confidence in its interpretation    14  Implementing Dialog module  • The parse method allows Dialog subclasses to implement their  own mechanism for handling raw strings of data  • The interpret method is responsible for interpreting an  incoming list of parsed sentences, updating the internal state  of the Dialog, and computing a confidence level for the  interpretation  • Finally, the respond method accepts interpreted sentences, a  confidence score, and arbitrary keyword arguments in order to  produce a text-based response based on the current state of  the Dialog    15  Implementing Dialog module    16  Module 9 – Section 3  Rule based Conversation     17  Rule based Greetings Module  • In this section, we will implement a rules-based greeting feature  inspired by these early models, which uses regular expressions to  match utterances.  • Our version will maintain state primarily to acknowledge  participants entering and leaving the dialog, and respond to them  with appropriate salutations and questions.  • The Greeting dialog implements our conversational framework by  extending the Dialog base class.  • At the heart of the Greeting dialog is a dictionary, PATTERNS,  stored as a class variable. This dictionary maps the kind of  interactions (described by key) to a regular expression that defines  the expected input for that interaction.  • In particular, our simple Greeting dialog is prepared for greetings,  introductions, goodbyes, and roll calls    18  Rule based Greetings module    19  Rule based Greetings module    20  Rule based Greetings module    21  Rule based Conversation module    22  Module 9 – Section 4  Question and Answer system    23  Building Question and Answer System  • One of the most common uses of chatbots is to quickly and easily  answer fact-based questions such as “How long is the Nile river?”  • There exists a variety of fact and knowledge bases on the web  such as DBPedia, Yago2, and Google Knowledge Graph  • The challenge is converting a natural language question into a  database query. While statistical matching of questions to their  answers is one simple mechanism for this, more robust  approaches use both statistical and semantic information;  • for example, using a frame-based approach to create templates  that can be derived into SPARQLqueries or using classification  techniques to identify the type of answer required (e.g., a location,  an amount, etc.)    24  Detecting Questions  • The first step of building this system is to detect when we’ve been  asked a question, and to determine what type of question it is.  • An excellent first step is to consider what questions look like  • Though questions are often posed in irregular and unexpected  ways, some patterns do exist.  • To detect these patterns, we will need to perform some type of  syntactic parsing—in other words, a mechanism that exploits  context-free grammars to systematically assign syntactic structure to  incoming text  • We have seen that NLTK has a number of grammer-based parsers,  but all require us to provide a grammar to specify the rules, which  will unnecessarily limit our chatbot’s flexibility  • Next, we will instead explore pretrained parsing model as more  flexible alternatives:  – dependency parsing  – constituency parsing    25  Dependency Parsing  • Dependency parsers are a lightweight mechanism to extract the  syntactic structure of a sentence by linking phrases together with  specific relationships.  • They do so by first identifying the head word of a phrase, then  establishing links between the words that modify the head.  • The result is an overlapping structure of arcs that identify  meaningful substructures of the sentence.  • Consider the sentence “How many teaspoons are in a  tablespoon?”. In Figure 9-3, we see a dependency parse as  visualized using SpaCy’s DisplaCy module  Fig-9.3: Spacy Dependency Tree    26  Implementing Dependency Parsing  • To recreate the parse shown in Figure 9-3, we first load SpaCy’s  prebuilt English language parsing model.  • We then write a function, plot_displacy_tree, that parses incoming  sentences using the prebuilt model and plots the resulting  dependency parse using the displacy.serve method.    27  Problem with Dependency Parsing  • Dependency parsers are extremely popular for producing fast and  correct grammatical analyses and when combined with part-of-  speech tagging, perform much of the work required for phrase-  level analysis.  • However, dependency parsing does not offer as rich and deep a  view of the structure of sentences, and as such, may not always  be sufficient or optimal.  • In our chatbot, we’ll demonstrate how to leverage a more  comprehensive tree representation, the constituency parsing.    28  Constituency Parsing  • Constituency parsing is a form of syntactic parsing whose goal is  to break down a sentence into nested phrase structures  • The output is a tree structure that captures complex  interrelationships between sub-phrases.  • Constituency parsers provide an opportunity to apply tree-  traversal algorithms that easily enable computation on text  • Here we can see a much more complex structure of sub-phrases  and more direct, unlabeled relationships between nodes in the  tree.    29  Constituency Parsing  • Constituency parse trees are comprised of terminal leaf nodes,  the part-of-speech tag, and the word itself  • The nonterminal nodes represent phrases that join the part-of-  speech tags into related groupings  • In our example constituency tree, the root phrase is an SBARQ, a  clause identified as a direct question because it is introduced by a  “wh”-word  Fig 9.4: Constituency parsing of “How many teaspoons are in a   tablespoon”    30  Implementing Constituency Parsing  • The Stanford CoreNLP package can be used to execute constituency  parsing  • NLTK made available a new module nltk.parse.stanford that enables us  to use the Stanford parsers from inside NLTK (assuming you have set up  the requisite .jars and PATH configuration) as follows:    31  Identifying Questions  • The pretrained models in SpaCy and CoreNLP give us a powerful  way to automatically parse and annotate input sentences.  • We can then use the annotations to traverse the parsed  sentences and look for part-of-speech tags that correspond to  questions.  • We can explore how different questions manifest using the Penn  Treebank tags, which we first encountered in “Part-of-Speech  Tagging” in module-2    32  Identifying Questions  • In our example, we can see from the root that our input is an  SBARQ (a direct question introduced by a “wh”-word), which in  this case is a WRB (a “wh”-adverb).    33  Identifying Questions  • The major advantage of using a technique like this for question detection  is the flexibility.  • For instance, if we change our question to “Sorry to trouble you, but how  many teaspoons are in a tablespoon?”, the output is different, but the  WHADJP and WRB question markers are still there    34  Penn Treebank II Tags for Question Detection    35  Implementing Question-Answer Module  • Now we will add to our chatbot is a question-and-answer system that  leverages the pretrained parsers to provide convenient kitchen  measurement conversions (questions about measurements as “How”  questions).  • For our question-type identification task, we will aim to be able to  interpret questions that take the form “How many X are in a Y?”    36  Implemention – Converter class  • The main class we write is Converter, which inherits the behavior of our  Dialog class.  • We need to initialize a Converter with a knowledge base of measurement  conversions, here a simple JSON file stored in CONVERSION_PATH and  containing all of the conversions between units of measure.  • We also initialize a parser (here we use CoreNLP), as well as a stemmer  from NLTK and an inflect.engine from the inflect library, which will enable us  to handle pluralization in the parse and respond methods, respectively.  • Our parse method will use the raw_parse method from CoreNLP to  generate constituency parses as demonstrated in the previous section    37  Implemention- Interpret method  • Next, in interpret method, we initialize a list to collect the  measures we want to convert from and to, an initial confidence  score of 0, and a dictionary to collect the results of our  interpretation  • We retrieve the root of the parsed sentence tree and scan  through the part-of-speech tags for ones that match the adverbial  phrase question pattern (WRB).  • If we identify any numbers (Tag CD: cardinal number) within the  question phrase subtree, we store that in our results dictionary as  the quantity for the target measure.  • Finally, we return a (results, confidence, kwargs) tuple, which the  respond method will use to determine whether and how to  respond to the user    38  Implemention- Interpret method    39  Implementation- Respond Method  • Before implementing our respond method, we need a few helper  utilities.  • The first is convert, which converts from the units of the source  measurement to those of the target measurement.  • We also need to add round, pluralize, and numericalize methods,  which leverage utilities from the humanize library to transform  numbers to more natural human-readable form  • Finally, in the respond method, we check to see if our confidence in  our interpretation is sufficiently high, and if so, we use convert to  perform the actual measurement conversions, and then round,  pluralize, and numericalize to ensure the final response is easy for the  user to read    40  Implementation- Respond Method    41  Implementation- Respond Method    42  Implementation- Listen Method  • Now we can experiment with using the listen method on a few  possible input questions to see how well our Converter class is able  to handle different combinations and quantities of source and target  units.    43  Module 9 – Section 5  Recommender system    44  Building Recipe Recommender module  • In this section we will be adding a module to our chatbot – a  recipe recommender.  • As shown in the pipeline, there will be two phase of this module:  – In Build phase, we need to train a recommender system through using a  classification algorithm and a cooking corpus  – In Deploy phase, it will suggest a recipe based on the ingredients during  conversation with a user.  Fig-9.5: Recipe Recommender Schema    45  Building Recommender model  • To train our recommender, we’ll be using a cooking corpus  comprised of blog posts and articles that contain recipes for specific  dishes as well as narratives and descriptions for those dishes.  • We will build the pipeline that will leverage a recipe corpus to perform  text normalization, vectorization, and dimensionality reduction, and  finally use a nearest-neighbor algorithm to provide recipe  recommendations.  • Usually the main challenges of using nearest-neighbor algorithms is  the complexity of search increases as dimensionality increases;  Hence, in case of text applications, dimensionality reduction is often  an important step.  • Also we will leverage a less computationally expensive alternative to  traditional unsupervised nearest-neighbor search – Ball tree  algorithm.    46  Implementation- BallTreerecommender  • First, we’ll create a BallTreeRecommender class, which is  initialized with a k for the desired number of recommendations  (which will default to 3), paths to a pickled fitted transformer  svd.pkl, and fitted ball tree tree.pkl.  • If the model has already been fit, these paths will exist, and the  load method will load them from disk for use.  • Otherwise, they will be fitted and saved using ScikitLearn’s  joblib serializer in our fit_transform method.  • Once a sklearn.neighbors.BallTree model has been fitted, we  can use the tree.query method that uses the fitted transformer  to vectorize and transform incoming text and return only the  indices for the closest recipes.    47  Implementation- BallTreerecommender    48  Implementation- BallTreerecommender    49  Recommending recipe during chat  • Once we have fit our BallTreeRecommender on our pickled recipe  corpus and saved the model artifacts, we can now implement the  recipe recommendations in the context of our Dialog abstract  base class.  • We will be reusing “HTML PickledCorpusReader” class we  defined earlier, except that we will be adding a titles() method that  will allow us to grab the page titles from each HTMLfile, which will  give us a human-intelligible way to refer to each recipe.    50  Recommending recipe during chat  • Our new class RecipeRecommender is instantiated with a pickled  corpus reader and an estimator that implements  BallTreeRecommender model.  • If the recommender isn’t already fitted, the __init__ method will  ensure that it is fit and transformed.  • Next, the parse method splits the input text string into a list and  performs part-of-speech tagging.  • The interpret method takes in the parsed text and determines  whether it is a list of ingredients. If so, it transforms the utterance into  a collection of nouns and then assigns a confidence score according  to the percent of the input text that is nouns.  • Finally, the respond method takes in the list of nouns extracted from  the interpret method as well as the confidence. If interpret has  successfully extracted a sufficient number of nouns from the input,  the confidence will be high enough to generate recommendations.    51  Recommending recipe during chat    52  Recommending recipe during chat  One Sample Result:    53  Module 9 – Section 5  Resources and Wrap-up    54  • Complete the notebook in the assignments section for this   week  Homework    55  • Scaling with Multiprocessing and Spark  • Reading:  Chapter 11 textbook  Next Class     56  Join the conversation with us online:  facebook.com/uoftscs  @uoftscs  linkedin.com/company/university-of-toronto-school-of-continuing-studies  @uoftscs  Follow us on social    57  Any questions?    58  Thank you for choosing the University of Toronto   School of Continuing Studies  Thank You   \n"
     ]
    }
   ],
   "source": [
    "with open(os.getcwd() + '/tika_PDF_comparison/3666_Module_9_Building_Chatbots.txt') as f:\n",
    "    tika_output = f.read()\n",
    "print(tika_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all have some differences both in utilization of code and in the output that is provided. Out of the three, the output we obtained from pdfminer was the cleanest and most easy to read. The outputs from PyPDF2 and Tika look similar. One of the main differences is that PyPDF2 output is a list with each page represented as a string. This can also be seen in the code as we have ended up going page by page. With Tika it can be seen that we did apply some heuristics in order to clean up the code a little. Our output ended up being a string. Through out this process, Linda favoured using pdfminer, while Rahim favoured using Tika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
